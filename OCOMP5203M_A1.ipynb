{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyshrey/ML/blob/main/OCOMP5203M_A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR4bovYL4CJz"
      },
      "source": [
        "## OCOM5203M Assignment 1 - Image Caption Generation [100 marks]\n",
        "\n",
        "The maximum number of marks for each part are shown in the section headers. As indicated in the main heading above, the overall assessment carries a maximum of 100 marks.\n",
        "\n",
        "This summative assessment is weighted 50% of the final grade for the module.\n",
        "\n",
        "### Motivation \n",
        "\n",
        "Through this coursework, you will:\n",
        "\n",
        "> 1. Understand the principles of text pre-processing and vocabulary building.\n",
        "> 2. Gain experience working with an image to text model.\n",
        "> 3. Use and compare two different text similarity metrics for evaluating an image to text model, and understand evaluation challenges.\n",
        "\n",
        "\n",
        "### Setup and resources \n",
        "\n",
        "Having a GPU will speed up the image feature extraction process. If you would like to use a GPU, please refer to the module website for recommended working environments with GPUs.\n",
        "\n",
        "Please implement the coursework using Python and PyTorch, and refer to the notebooks and exercises provided.\n",
        "\n",
        "This coursework will use a subset of the [COCO \"Common Objects in Context\" dataset](https://cocodataset.org/) for image caption generation. COCO contains 330K images, of 80 object categories, and at least five textual reference captions per image. Our subset consists of 5029 of these images, each of which has five or more different descriptions of the salient entities and activities, and we will refer to it as COCO_5029.\n",
        "\n",
        "To download the data:\n",
        "\n",
        "> 1. **Images**: download the zip file \"coco_subset_images.zip (812MB)\" [here](https://leeds365-my.sharepoint.com/:f:/g/personal/busmnom_leeds_ac_uk/EuAH3b6a4g9IjTNhroLLXPoB6ho6cwxYSNh885ZzrktYZA?e=QGSYpf).\n",
        "> 2. **Reference captions**: on the COCO [download page](https://cocodataset.org/#download), download the file named \"2017 Train/Val annotations (241MB)\". \n",
        "> 3. **Image meta data**: as our set is a subset of full COCO dataset, we have created a CSV file containing relevant meta data for our particular subset of images. You can download it also from Drive, \"coco_subset_meta.csv\" at the same link as 1.\n",
        "\n",
        "\n",
        "### Submission\n",
        "\n",
        "Please submit the following:\n",
        "\n",
        "> 1. Your completed Jupyter notebook file, in .ipynb format. **Do not change the file name or the automatic grading will be affected.**\n",
        "> 2. The .html version of your notebook; File > Download as > HTML (.html). Check that all cells have been run and all outputs (including all graphs you would like to be marked) displayed in the .html for marking.\n",
        "\n",
        "\n",
        "Final note:\n",
        "\n",
        "> **Please include in this notebook everything that you would like to be marked, including figures. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PApXdyNO5hZM"
      },
      "source": [
        "Your student username (for example, ```sc15jb```):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUcMD-ID5hZN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oyl5a1Rn5hZN"
      },
      "source": [
        "Your full name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Otx0SkGQ5hZO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uK5tX4y5hZO"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Feel free to add to this section as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qfkKJeYp5hZP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtKA4RWm5hZP"
      },
      "source": [
        "Detect which device (CPU/GPU) to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8koeiZxo5hZQ",
        "outputId": "0b137cc4-840d-42f7-8101-24db6d4e55f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g-MbowuL5v5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fViBLV05hZQ"
      },
      "source": [
        "The basic principle of our image-to-text model is as pictured in the diagram below, where an Encoder network encodes the input image as a feature vector by providing the outputs of the last fully-connected layer of a pre-trained CNN (we use [ResNet-152](https://arxiv.org/abs/1512.03385)). This pretrained network has been trained on the complete ImageNet dataset and is thus able to recognise common objects. \n",
        "\n",
        "These features are then fed into a Decoder network along with the reference captions. As the image feature dimensions are large and sparse, the Decoder network includes a linear layer which downsizes them, followed by a batch normalisation layer to speed up training. Those resulting features, as well as the reference text captions, are passed into a recurrent network (we will use an RNN). \n",
        "\n",
        "The reference captions used to compute loss are represented as numerical vectors via an embedding layer whose weights are learned during training.\n",
        "\n",
        "![Encoder Decoder](encoder_decoder_diagramv2022.png)\n",
        "\n",
        "The Encoder-Decoder network could be coupled and trained end-to-end, without saving features to disk; however, this requires iterating through the entire image training set during training. We can make the training more efficient by decoupling the networks. \n",
        "\n",
        "We will first extract the feature representations of the images from the Encoder and save them (Part 1). During training of the Decoder (Part 3), we only need to iterate over the image feature data and the reference captions.\n",
        "\n",
        "### Overview\n",
        "\n",
        "> 1. Extracting image features \n",
        "> 2. Text preparation of training and validation data \n",
        "> 3. Training the decoder\n",
        "> 4. Generating predictions on test data\n",
        "> 5. Caption evaluation via BLEU score\n",
        "> 6. Caption evaluation via Cosine similarity\n",
        "> 7. Comparing BLEU and Cosine similarity\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCauGq4N5hZR"
      },
      "source": [
        "## 1 Extracting image features [11 marks]\n",
        "\n",
        "### 1.1 EncoderCNN\n",
        "\n",
        "Read through the template EncoderCNN class below and complete the class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_BjbyYQm5hZR"
      },
      "outputs": [],
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
        "        # print(modules)\n",
        "\n",
        "        # TO COMPLETE\n",
        "        # keep all layers of the pretrained net except the last one\n",
        "\n",
        "        \n",
        "    def forward(self, images):\n",
        "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
        "        with torch.no_grad():\n",
        "          features = self.resnet(images)\n",
        "        features = features.reshape(features.size(0), -1)\n",
        "        features = self.bn(self.linear(features))\n",
        "        return features\n",
        "        # TO COMPLETE\n",
        "        # remember no gradients are needed\n",
        "        # return features "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "e3f05823251746e8b902c356d296786a",
            "0c955e3606a9457b982ce79a497d21db",
            "b97613fd9a2d420681ec239049f95ac0",
            "df8ecf1a419047f38e93178ca87e39c3",
            "9d422597377840029a87e27ec88b0e29",
            "4545a1581d0d422ebc98fdcdf78ee932",
            "7178f76e5793444ebd37b60fe0b6d183",
            "34e7b4940b94424992593c660185cae0",
            "17a768df79484d85902d76a37ccd9ce6",
            "b258bcf81fcd4f7186e962baf655df6e",
            "fca7a0f6ede946e396241c0aa609cd5f"
          ]
        },
        "id": "myxuSAuq5hZS",
        "outputId": "9d663e29-1c7d-4a43-e4bd-227aec8d374b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/230M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3f05823251746e8b902c356d296786a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderCNN()"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# instantiate encoder and put into evaluation mode.\n",
        "encoder = EncoderCNN().to(device)\n",
        "encoder.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGbBvd295hZS"
      },
      "source": [
        "### 1.2 Processing the images\n",
        "\n",
        "Pass the images through the ```Encoder``` model, saving the resulting features for each image. You may like to use a ```Dataset``` and ```DataLoader``` to load the data in batches for faster processing, or you may choose to simply read in one image at a time from disk without any loaders.\n",
        "\n",
        "Note that as this is a forward pass only, no gradients are needed. You will need to be able to match each image ID (the image name without file extension) with its features later, so we suggest either saving a dictionary of image ID: image features, or keeping a separate ordered list of image IDs.\n",
        "\n",
        "Use this ImageNet transform provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkW0ym-05hZT"
      },
      "outputs": [],
      "source": [
        "data_transform = transforms.Compose([ \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(224), \n",
        "    transforms.CenterCrop(224), \n",
        "    transforms.Normalize((0.485, 0.456, 0.406),   # using ImageNet norms\n",
        "                         (0.229, 0.224, 0.225))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8RQlINq5hZT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCcgyZuA5hZT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p6OXAmo5hZU"
      },
      "outputs": [],
      "source": [
        "# torch.save(features, 'features.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfR--uYXHdIi"
      },
      "source": [
        "## 2 Text preparation [23 marks]\n",
        "\n",
        "\n",
        "### 2.1 Build the caption dataset\n",
        "\n",
        "All our selected COCO_5029 images are from the official 2017 train set.\n",
        "\n",
        "The ```coco_subset_meta.csv``` file includes the image filenames and unique IDs of all the images in our subset. The ```id``` column corresponds to each unique image ID.\n",
        "\n",
        "The COCO dataset includes many different types of annotations: bounding boxes, keypoints, reference captions, and more. We are interested in the captioning labels. Open ```captions_train2017.json``` from the zip file downloaded from the COCO website. You are welcome to come up with your own way of doing it, but we recommend using the ```json``` package to initially inspect the data, then the ```pandas``` package to look at the annotations (if you read in the file as ```data```, then you can access the annotations dictionary as ```data['annotations']```).\n",
        "\n",
        "Use ```coco_subset_meta.csv``` to cross-reference with the annotations from ```captions_train2017.json``` to get all the reference captions for each image in COCO_5029.\n",
        "\n",
        "For example, you may end up with data looking like this (this is a ```pandas``` DataFrame, but it could also be several lists, or some other data structure/s):\n",
        "\n",
        "<img src=\"df_caption_set.png\" alt=\"images matched to caption\" width=\"700\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DUtQYH8S9V6O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPWN96uh5hZV"
      },
      "outputs": [],
      "source": [
        "#connecting to google drive to get the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ihn6riA5hZV",
        "outputId": "d077e921-439b-46e2-e9de-0c27ddb99a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eRnyf4Xq9PGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B59YxE0U9PO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "03TW4oL39PSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8LzztnFk9PU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ak0nqwH_9PYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5Tt7IM25hZV"
      },
      "source": [
        "### 2.2 Clean the captions\n",
        "\n",
        "Create a cleaned version of each caption. If using dataframes, we suggest saving the cleaned captions in a new column; otherwise, if you are storing your data in some other way, create data structures as needed. \n",
        "\n",
        "**A cleaned caption should be all lowercase, and consist of only alphabet characters.**\n",
        "\n",
        "Print out 10 original captions next to their cleaned versions to facilitate marking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PFXEvpE5hZW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8oTXenS5hZW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Umb4VT5hZW"
      },
      "source": [
        "### 2.3  Split the data\n",
        "\n",
        "Split the data 70/10/20% into train/validation/test sets. **Be sure that each unique image (and all corresponding captions) only appear in a single set.**\n",
        "\n",
        "We provide the function below which, given a list of unique image IDs and a 3-split ratio, shuffles and returns  a split of the image IDs.\n",
        "\n",
        "If using a dataframe, ```df['image_id'].unique()``` will return the list of unique image IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rue1LnaX5hZW"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "def split_ids(image_id_list, train=.7, valid=0.1, test=0.2):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        image_id_list (int list): list of unique image ids\n",
        "        train (float): train split size (between 0 - 1)\n",
        "        valid (float): valid split size (between 0 - 1)\n",
        "        test (float): test split size (between 0 - 1)\n",
        "    \"\"\"\n",
        "    list_copy = image_id_list.copy()\n",
        "    random.shuffle(list_copy)\n",
        "    \n",
        "    train_size = math.floor(len(list_copy) * train)\n",
        "    valid_size = math.floor(len(list_copy) * valid)\n",
        "    \n",
        "    return list_copy[:train_size], list_copy[train_size:(train_size + valid_size)], list_copy[(train_size + valid_size):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6XWiygT5hZX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggrsNGPk5hZX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3msz-_a5hZX"
      },
      "source": [
        "### 2.3 Building the vocabulary\n",
        "\n",
        "The vocabulary consists of all the possible words which can be used - both as input into the model, and as output predictions, and we will build it using the cleaned words found in the reference captions from the training set. In the vocabulary each unique word is mapped to a unique integer (a Python ```dictionary``` object).\n",
        "\n",
        "A ```Vocabulary``` object is provided for you below to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZDBfoa55hZY"
      },
      "outputs": [],
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\" Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
        "    def __init__(self):\n",
        "        # intially, set both the IDs and words to dictionaries with special tokens\n",
        "        self.word2idx = {'<pad>': 0, '<unk>': 1, '<end>': 2}\n",
        "        self.idx2word = {0: '<pad>', 1: '<unk>', 2: '<end>'}\n",
        "        self.idx = 3\n",
        "\n",
        "    def add_word(self, word):\n",
        "        # if the word does not already exist in the dictionary, add it\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            # increment the ID for the next word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        # if we try to access a word not in the dictionary, return the id for <unk>\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mxFuzhj5hZY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvs5b6M95hZY"
      },
      "source": [
        "Collect all words from the cleaned captions in the **training and validation sets**, ignoring any words which appear 3 times or less; this should leave you with roughly 2200 words (plus or minus is fine). As the vocabulary size affects the embedding layer dimensions, it is better not to add the very infrequently used words to the vocabulary.\n",
        "\n",
        "Create an instance of the ```Vocabulary()``` object and add all your words to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bPL4WQg5hZY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mzn9S_cv5hZY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uis6nR9N5hZZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYx-qVAX5hZZ"
      },
      "source": [
        "### 2.4 The Dataset and DataLoader\n",
        "\n",
        "Create a PyTorch ```Dataset``` class and a corresponding ```DataLoader``` for the inputs to the decoder. Create three sets: one each for training, validation, and test. Set ```shuffle=True``` for the training set DataLoader.\n",
        "\n",
        "The ```Dataset``` function ```__getitem__(self, index)``` should return three Tensors:\n",
        "\n",
        ">1. A Tensor of image features, dimension (1, 2048).\n",
        ">2. A Tensor of integer word ids representing the reference caption; use your ```Vocabulary``` object to convert each word in the caption to a word ID. Be sure to add the word ID for the ```<end>``` token at the end of each caption, then fill in the the rest of the caption with the ```<pad>``` token so that each caption has uniform lenth (max sequence length) of **47**.\n",
        ">3. A Tensor of integers representing the true lengths of every caption in the batch (include the ```<end>``` token in the count).\n",
        "\n",
        "\n",
        "Note that as each unique image has five or more (say, ```n```) reference captions, each image feature will appear ```n``` times, once in each unique (feature, caption) pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkIvy-SM5hZZ"
      },
      "outputs": [],
      "source": [
        "class COCO_Subset(Dataset):\n",
        "    \"\"\" COCO subset custom dataset, compatible with torch.utils.data.DataLoader. \"\"\"\n",
        "    \n",
        "    def __init__(self, df, features, vocab):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df: (dataframe or some other data structure/s you may prefer to use)\n",
        "            features: image features\n",
        "            vocab: vocabulary wrapper\n",
        "           \n",
        "        \"\"\"\n",
        "        \n",
        "        # TO COMPLETE\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Returns one data tuple (feature [1, 2048], target caption of word IDs [1, 47], and integer true caption length) \"\"\"   \n",
        "        \n",
        "       # TO COMPLETE\n",
        "    \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgkEEmPA5hZZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S66p-6d5hZZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2pYh9Rf5hZZ"
      },
      "source": [
        "Load one batch of the training set and print out the shape of each returned Tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVEaAwCH5hZZ"
      },
      "outputs": [],
      "source": [
        "# train_iter = iter(train_loader)\n",
        "# features, captions, lengths = train_iter.next()\n",
        "# print(features.shape)\n",
        "# print(captions.shape)\n",
        "# print(lengths.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ3naDXD5hZa"
      },
      "source": [
        "## 3 Train DecoderRNN [15 marks]\n",
        "\n",
        "### 3.1 Define the encoder model\n",
        "\n",
        "Read through the ```DecoderRNN``` model below. First, complete the decoder by adding an ```rnn``` layer to the decoder where indicated, using [the PyTorch API as reference](https://pytorch.org/docs/stable/nn.html#rnn).\n",
        "\n",
        "Keep all the default parameters except for ```batch_first```, which you may set to True.\n",
        "\n",
        "In particular, understand the meaning of ```pack_padded_sequence()``` as used in ```forward()```. Refer to the [PyTorch ```pack_padded_sequence()``` documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFbFxvIU5hZa"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=256, hidden_size=512, num_layers=1, max_seq_length=47):\n",
        "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        # we want a specific output size, which is the size of our embedding, so\n",
        "        # we feed our extracted features from the last fc layer (dimensions 1 x 2048)\n",
        "        # into a Linear layer to resize\n",
        "        self.resize = nn.Linear(2048, embed_size)\n",
        "        \n",
        "        # batch normalisation helps to speed up training\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "\n",
        "        # What is an embedding layer?\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # TO COMPLETE\n",
        "        # self.rnn = \n",
        "\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.max_seq_length = max_seq_length\n",
        "        \n",
        "\n",
        "    def forward(self, features, captions, lengths):\n",
        "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
        "        embeddings = self.embed(captions)\n",
        "        im_features = self.resize(features)\n",
        "        im_features = self.bn(im_features)\n",
        "        \n",
        "        embeddings = torch.cat((im_features.unsqueeze(1), embeddings), 1)\n",
        "    \n",
        "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False) \n",
        "        # pack_padded_sequence returns a PackedSequence object, which contains two items: \n",
        "        # the packed data (data cut off at its true length and flattened into one list), and \n",
        "        # the batch_sizes, or the number of elements at each sequence step in the batch.\n",
        "        # For instance, given data [a, b, c] and [x] the PackedSequence would contain data \n",
        "        # [a, x, b, c] with batch_sizes=[2,1,1].\n",
        "        \n",
        "        hiddens, _ = self.rnn(packed)\n",
        "        outputs = self.linear(hiddens.data) # hiddens[0]\n",
        "        return outputs\n",
        "    \n",
        "    \n",
        "    def sample(self, features, states=None):\n",
        "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
        "        sampled_ids = []\n",
        "\n",
        "        inputs = self.bn(self.resize(features)).unsqueeze(1)\n",
        "        for i in range(self.max_seq_length):\n",
        "            hiddens, states = self.rnn(inputs, states)  # hiddens: (batch_size, 1, hidden_size)\n",
        "            outputs = self.linear(hiddens.squeeze(1))   # outputs:  (batch_size, vocab_size)\n",
        "            _, predicted = outputs.max(1)               # predicted: (batch_size)\n",
        "            sampled_ids.append(predicted)\n",
        "            inputs = self.embed(predicted)              # inputs: (batch_size, embed_size)\n",
        "            inputs = inputs.unsqueeze(1)                # inputs: (batch_size, 1, embed_size)\n",
        "        sampled_ids = torch.stack(sampled_ids, 1)       # sampled_ids: (batch_size, max_seq_length)\n",
        "        return sampled_ids\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCLsCeEB5hZa"
      },
      "outputs": [],
      "source": [
        "# instantiate decoder\n",
        "decoder = DecoderRNN(len(vocab)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga9bFPnu5hZa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll5JI91J5hZb"
      },
      "source": [
        "### 3.2 Train the decoder\n",
        "\n",
        "Train the decoder by passing the features, reference captions, and targets to the decoder, then computing loss based on the outputs and the targets. Note that when passing the targets and model outputs to the loss function, the targets will also need to be formatted using ```pack_padded_sequence()```.\n",
        "\n",
        "We recommend a batch size of around 64 (though feel free to adjust as necessary for your hardware).\n",
        "\n",
        "**We strongly recommend saving a checkpoint of your trained model after training so you don't need to re-train multiple times.**\n",
        "\n",
        "Display a graph of training and validation loss over epochs to justify your stopping point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmVVrKd45hZb"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib0LCGj_5hZb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evsh9miO5hZb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUqfK8QI5hZb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXzlLbstCE7f"
      },
      "source": [
        "## 4 Generate predictions on test data [8 marks]\n",
        "\n",
        "Display 5 sample test images containing different objects, along with your model’s generated captions and all the reference captions for each.\n",
        "\n",
        "> Remember that everything **displayed** in the submitted notebook and .html file will be marked, so be sure to run all relevant cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-O-Vz2l5hZb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBU8-UsD5hZc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7np733bd5hZc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVfEgbC4I_dE"
      },
      "source": [
        "## 5 Caption evaluation using BLEU score [10 marks]\n",
        "\n",
        "There are different methods for measuring the performance of image to text models. We will evaluate our model by measuring the text similarity between the generated caption and the reference captions, using two commonly used methods. Ther first method is known as *Bilingual Evaluation Understudy (BLEU)*.\n",
        "\n",
        "###  5.1 BLEU score\n",
        "\n",
        "\n",
        "One common way of comparing a generated text to a reference text is using BLEU. This article gives a good intuition to how the BLEU score is computed: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/, and you may find an implementation online to use. One option is the NLTK implementation `nltk.translate.bleu_score` here: https://www.nltk.org/api/nltk.translate.bleu_score.html\n",
        "\n",
        "\n",
        "> **Tip:** BLEU scores can be weighted by ith-gram. Check that your scores make sense; and feel free to use a weighting that best matches the data. We will not be looking for specific score ranges; rather we will check that the scores are reasonable and meaningful given the captions.\n",
        "\n",
        "Write the code to evaluate the trained model on the complete test set and calculate the BLEU score using the predictions, compared against all five references captions. \n",
        "\n",
        "Display a histogram of the distribution of scores over the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xypfUN7y4CKI"
      },
      "outputs": [],
      "source": [
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAXrOzjE5hZc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPnCbFp25hZc"
      },
      "source": [
        "### 5.2 BLEU score examples\n",
        "\n",
        "Find one sample with high BLEU score and one with a low score, and display the model's predicted sentences, the BLEU scores, and the 5 reference captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3RqgXUq5hZc"
      },
      "outputs": [],
      "source": [
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIuM7H5Q5hZc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VQKNo384CKP"
      },
      "source": [
        "## 6 Caption evaluation using cosine similarity [12 marks]\n",
        "\n",
        "###  6.1 Cosine similarity\n",
        "\n",
        "The cosine similarity measures the cosine of the angle between two vectors in n-dimensional space. The smaller the angle, the greater the similarity.\n",
        "\n",
        "To use the cosine similarity to measure the similarity between the generated caption and the reference captions: \n",
        "\n",
        "* Find the embedding vector of each word in the caption \n",
        "* Compute the average vector for each caption \n",
        "* Compute the cosine similarity score between the average vector of the generated caption and average vector of each reference caption\n",
        "* Compute the average of these scores \n",
        "\n",
        "Calculate the cosine similarity using the model's predictions over the whole test set. \n",
        "\n",
        "Display a histogram of the distribution of scores over the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4kaOxNY5hZd"
      },
      "outputs": [],
      "source": [
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aECpoyvk5hZd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19TErXv45hZd"
      },
      "source": [
        "#### 6.2 Cosine similarity examples \n",
        "\n",
        "Find one sample with high cosine similarity score and one with a low score, and display the model's predicted sentences, the cosine similarity scores, and the 5 reference captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRINvQc-5hZd"
      },
      "outputs": [],
      "source": [
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbVvSjcW5hZd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDmwrp-w4CKR"
      },
      "source": [
        "## 7 Comparing BLEU and Cosine similarity [16 marks]\n",
        "\n",
        "### 7.1 Test set distribution of scores\n",
        "\n",
        "Compare the model’s performance on the test set evaluated using BLEU and cosine similarity and discuss some weaknesses and strengths of each method (explain in words, in a text box below). \n",
        "\n",
        "Please note, to compare the average test scores, you need to rescale the Cosine similarity scores [-1 to 1] to match the range of BLEU method [0.0 - 1.0]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2O8TZG74CKS"
      },
      "outputs": [],
      "source": [
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qigb0A9F4CKV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2fHKGoo5hZe"
      },
      "source": [
        " ### 7.2 Analysis of individual examples\n",
        " \n",
        "Find and display one example where both methods give similar scores and another example where they do not and discuss. Include both scores, predicted captions, and reference captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixoMFjun5hZe"
      },
      "outputs": [],
      "source": [
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHFyGELx5hZe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPb2D3az5hZe"
      },
      "source": [
        "### Overall quality [5 marks]\n",
        "\n",
        "See the top of the notebook for submission instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK_S95Rk5hZf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3f05823251746e8b902c356d296786a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c955e3606a9457b982ce79a497d21db",
              "IPY_MODEL_b97613fd9a2d420681ec239049f95ac0",
              "IPY_MODEL_df8ecf1a419047f38e93178ca87e39c3"
            ],
            "layout": "IPY_MODEL_9d422597377840029a87e27ec88b0e29"
          }
        },
        "0c955e3606a9457b982ce79a497d21db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4545a1581d0d422ebc98fdcdf78ee932",
            "placeholder": "​",
            "style": "IPY_MODEL_7178f76e5793444ebd37b60fe0b6d183",
            "value": "100%"
          }
        },
        "b97613fd9a2d420681ec239049f95ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34e7b4940b94424992593c660185cae0",
            "max": 241627721,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17a768df79484d85902d76a37ccd9ce6",
            "value": 241627721
          }
        },
        "df8ecf1a419047f38e93178ca87e39c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b258bcf81fcd4f7186e962baf655df6e",
            "placeholder": "​",
            "style": "IPY_MODEL_fca7a0f6ede946e396241c0aa609cd5f",
            "value": " 230M/230M [00:01&lt;00:00, 246MB/s]"
          }
        },
        "9d422597377840029a87e27ec88b0e29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4545a1581d0d422ebc98fdcdf78ee932": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7178f76e5793444ebd37b60fe0b6d183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34e7b4940b94424992593c660185cae0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17a768df79484d85902d76a37ccd9ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b258bcf81fcd4f7186e962baf655df6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fca7a0f6ede946e396241c0aa609cd5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}