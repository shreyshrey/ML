{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyshrey/ML/blob/main/OCOMP5203M_A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR4bovYL4CJz"
      },
      "source": [
        "## OCOM5203M Assignment 1 - Image Caption Generation [100 marks]\n",
        "\n",
        "The maximum number of marks for each part are shown in the section headers. As indicated in the main heading above, the overall assessment carries a maximum of 100 marks.\n",
        "\n",
        "This summative assessment is weighted 50% of the final grade for the module.\n",
        "\n",
        "### Motivation \n",
        "\n",
        "Through this coursework, you will:\n",
        "\n",
        "> 1. Understand the principles of text pre-processing and vocabulary building.\n",
        "> 2. Gain experience working with an image to text model.\n",
        "> 3. Use and compare two different text similarity metrics for evaluating an image to text model, and understand evaluation challenges.\n",
        "\n",
        "\n",
        "### Setup and resources \n",
        "\n",
        "Having a GPU will speed up the image feature extraction process. If you would like to use a GPU, please refer to the module website for recommended working environments with GPUs.\n",
        "\n",
        "Please implement the coursework using Python and PyTorch, and refer to the notebooks and exercises provided.\n",
        "\n",
        "This coursework will use a subset of the [COCO \"Common Objects in Context\" dataset](https://cocodataset.org/) for image caption generation. COCO contains 330K images, of 80 object categories, and at least five textual reference captions per image. Our subset consists of 5029 of these images, each of which has five or more different descriptions of the salient entities and activities, and we will refer to it as COCO_5029.\n",
        "\n",
        "To download the data:\n",
        "\n",
        "> 1. **Images**: download the zip file \"coco_subset_images.zip (812MB)\" [here](https://leeds365-my.sharepoint.com/:f:/g/personal/busmnom_leeds_ac_uk/EuAH3b6a4g9IjTNhroLLXPoB6ho6cwxYSNh885ZzrktYZA?e=QGSYpf).\n",
        "> 2. **Reference captions**: on the COCO [download page](https://cocodataset.org/#download), download the file named \"2017 Train/Val annotations (241MB)\". \n",
        "> 3. **Image meta data**: as our set is a subset of full COCO dataset, we have created a CSV file containing relevant meta data for our particular subset of images. You can download it also from Drive, \"coco_subset_meta.csv\" at the same link as 1.\n",
        "\n",
        "\n",
        "### Submission\n",
        "\n",
        "Please submit the following:\n",
        "\n",
        "> 1. Your completed Jupyter notebook file, in .ipynb format. **Do not change the file name or the automatic grading will be affected.**\n",
        "> 2. The .html version of your notebook; File > Download as > HTML (.html). Check that all cells have been run and all outputs (including all graphs you would like to be marked) displayed in the .html for marking.\n",
        "\n",
        "\n",
        "Final note:\n",
        "\n",
        "> **Please include in this notebook everything that you would like to be marked, including figures. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PApXdyNO5hZM"
      },
      "source": [
        "Your student username (for example, ```sc15jb```):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUcMD-ID5hZN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oyl5a1Rn5hZN"
      },
      "source": [
        "Your full name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Otx0SkGQ5hZO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uK5tX4y5hZO"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Feel free to add to this section as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qfkKJeYp5hZP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtKA4RWm5hZP"
      },
      "source": [
        "Detect which device (CPU/GPU) to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8koeiZxo5hZQ",
        "outputId": "b570df60-c089-4d91-f9e4-784ef43d6260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g-MbowuL5v5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fViBLV05hZQ"
      },
      "source": [
        "The basic principle of our image-to-text model is as pictured in the diagram below, where an Encoder network encodes the input image as a feature vector by providing the outputs of the last fully-connected layer of a pre-trained CNN (we use [ResNet-152](https://arxiv.org/abs/1512.03385)). This pretrained network has been trained on the complete ImageNet dataset and is thus able to recognise common objects. \n",
        "\n",
        "These features are then fed into a Decoder network along with the reference captions. As the image feature dimensions are large and sparse, the Decoder network includes a linear layer which downsizes them, followed by a batch normalisation layer to speed up training. Those resulting features, as well as the reference text captions, are passed into a recurrent network (we will use an RNN). \n",
        "\n",
        "The reference captions used to compute loss are represented as numerical vectors via an embedding layer whose weights are learned during training.\n",
        "\n",
        "![Encoder Decoder](encoder_decoder_diagramv2022.png)\n",
        "\n",
        "The Encoder-Decoder network could be coupled and trained end-to-end, without saving features to disk; however, this requires iterating through the entire image training set during training. We can make the training more efficient by decoupling the networks. \n",
        "\n",
        "We will first extract the feature representations of the images from the Encoder and save them (Part 1). During training of the Decoder (Part 3), we only need to iterate over the image feature data and the reference captions.\n",
        "\n",
        "### Overview\n",
        "\n",
        "> 1. Extracting image features \n",
        "> 2. Text preparation of training and validation data \n",
        "> 3. Training the decoder\n",
        "> 4. Generating predictions on test data\n",
        "> 5. Caption evaluation via BLEU score\n",
        "> 6. Caption evaluation via Cosine similarity\n",
        "> 7. Comparing BLEU and Cosine similarity\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCauGq4N5hZR"
      },
      "source": [
        "## 1 Extracting image features [11 marks]\n",
        "\n",
        "### 1.1 EncoderCNN\n",
        "\n",
        "Read through the template EncoderCNN class below and complete the class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_BjbyYQm5hZR"
      },
      "outputs": [],
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
        "        # print(modules)\n",
        "\n",
        "        # TO COMPLETE\n",
        "        # keep all layers of the pretrained net except the last one\n",
        "\n",
        "        \n",
        "    def forward(self, images):\n",
        "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
        "        with torch.no_grad():\n",
        "          features = self.resnet(images)\n",
        "        features = features.reshape(features.size(0), -1)\n",
        "        features = self.bn(self.linear(features))\n",
        "        return features\n",
        "        # TO COMPLETE\n",
        "        # remember no gradients are needed\n",
        "        # return features "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "6405531de5294830a30ef1432477ca57",
            "f523702c2d234f78b65b9893c6fc2992",
            "c8bf132ab08041e5b3a8552f86b1fd49",
            "1222b94434b7480798edc5671bd22f85",
            "a3b9a8a2f1fc412e8e87f6d8c6e547c4",
            "a069b7fd02164fcc8c9fa9a3f0c1f873",
            "b91188e4889f4238929c218967e2adbc",
            "494fc511ed5042c3a18e2711179daba3",
            "107ba6832156498f8b538a0b66b4a316",
            "7f74db6225ff48c49cd8bce99186f06f",
            "b5ef3d0bd8204a619f4dea96940af541"
          ]
        },
        "id": "myxuSAuq5hZS",
        "outputId": "6eb5f70f-2daf-41d3-ca42-107e51079881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/230M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6405531de5294830a30ef1432477ca57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderCNN()"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# instantiate encoder and put into evaluation mode.\n",
        "encoder = EncoderCNN().to(device)\n",
        "encoder.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGbBvd295hZS"
      },
      "source": [
        "### 1.2 Processing the images\n",
        "\n",
        "Pass the images through the ```Encoder``` model, saving the resulting features for each image. You may like to use a ```Dataset``` and ```DataLoader``` to load the data in batches for faster processing, or you may choose to simply read in one image at a time from disk without any loaders.\n",
        "\n",
        "Note that as this is a forward pass only, no gradients are needed. You will need to be able to match each image ID (the image name without file extension) with its features later, so we suggest either saving a dictionary of image ID: image features, or keeping a separate ordered list of image IDs.\n",
        "\n",
        "Use this ImageNet transform provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkW0ym-05hZT"
      },
      "outputs": [],
      "source": [
        "data_transform = transforms.Compose([ \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(224), \n",
        "    transforms.CenterCrop(224), \n",
        "    transforms.Normalize((0.485, 0.456, 0.406),   # using ImageNet norms\n",
        "                         (0.229, 0.224, 0.225))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8RQlINq5hZT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCcgyZuA5hZT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p6OXAmo5hZU"
      },
      "outputs": [],
      "source": [
        "# torch.save(features, 'features.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfR--uYXHdIi"
      },
      "source": [
        "## 2 Text preparation [23 marks]\n",
        "\n",
        "\n",
        "### 2.1 Build the caption dataset\n",
        "\n",
        "All our selected COCO_5029 images are from the official 2017 train set.\n",
        "\n",
        "The ```coco_subset_meta.csv``` file includes the image filenames and unique IDs of all the images in our subset. The ```id``` column corresponds to each unique image ID.\n",
        "\n",
        "The COCO dataset includes many different types of annotations: bounding boxes, keypoints, reference captions, and more. We are interested in the captioning labels. Open ```captions_train2017.json``` from the zip file downloaded from the COCO website. You are welcome to come up with your own way of doing it, but we recommend using the ```json``` package to initially inspect the data, then the ```pandas``` package to look at the annotations (if you read in the file as ```data```, then you can access the annotations dictionary as ```data['annotations']```).\n",
        "\n",
        "Use ```coco_subset_meta.csv``` to cross-reference with the annotations from ```captions_train2017.json``` to get all the reference captions for each image in COCO_5029.\n",
        "\n",
        "For example, you may end up with data looking like this (this is a ```pandas``` DataFrame, but it could also be several lists, or some other data structure/s):\n",
        "\n",
        "<img src=\"df_caption_set.png\" alt=\"images matched to caption\" width=\"700\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DUtQYH8S9V6O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPWN96uh5hZV"
      },
      "outputs": [],
      "source": [
        "#connecting to google drive to get the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ihn6riA5hZV",
        "outputId": "65d65d1b-95e3-452c-a77e-9c7d0c3bdd45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "annotation_path = '/content/drive/MyDrive/Leeds MSc/DL/annotations'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta_file = pd.read_csv('/content/drive/MyDrive/Leeds MSc/DL/coco_subset_meta.csv')"
      ],
      "metadata": {
        "id": "eRnyf4Xq9PGR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = meta_file[['file_name','id']]\n",
        "df.rename(columns = {'id':'image_id'}, inplace = True)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "0bZjVzuMKTn6",
        "outputId": "4fd39027-a9b4-4da7-db6a-ded6ba9685b7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:5039: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return super().rename(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          file_name  image_id\n",
              "0  000000262145.jpg    262145\n",
              "1  000000262146.jpg    262146\n",
              "2  000000524291.jpg    524291\n",
              "3  000000262148.jpg    262148\n",
              "4  000000393223.jpg    393223"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aa16192e-9462-4f83-b822-b29011ede8ae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>image_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000000262145.jpg</td>\n",
              "      <td>262145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000000262146.jpg</td>\n",
              "      <td>262146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000000524291.jpg</td>\n",
              "      <td>524291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000000262148.jpg</td>\n",
              "      <td>262148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>000000393223.jpg</td>\n",
              "      <td>393223</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa16192e-9462-4f83-b822-b29011ede8ae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aa16192e-9462-4f83-b822-b29011ede8ae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aa16192e-9462-4f83-b822-b29011ede8ae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['image_id'] == 203564]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "YZzJiEG5ZkW_",
        "outputId": "132dbc15-f66a-4838-d572-11cb842e1793"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [file_name, image_id]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a71e6d7-008f-46e8-a914-b5cf2256f2f2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>image_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a71e6d7-008f-46e8-a914-b5cf2256f2f2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6a71e6d7-008f-46e8-a914-b5cf2256f2f2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6a71e6d7-008f-46e8-a914-b5cf2256f2f2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "  \n",
        "# Opening JSON file\n",
        "f = open('/content/drive/MyDrive/Leeds MSc/DL/annotations/captions_train2017.json')\n",
        "  \n",
        "# returns JSON object as \n",
        "# a dictionary\n",
        "data = json.load(f)\n",
        "\n",
        "print(data.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B59YxE0U9PO7",
        "outputId": "3032ce37-3cbb-49bd-925d-c202027efd8d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['info', 'licenses', 'images', 'annotations'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ann_list = data['annotations']\n",
        "ann_df = pd.DataFrame.from_records(ann_list)\n",
        "ann_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03TW4oL39PSC",
        "outputId": "1890d4e6-d949-4fd9-93ea-413722c90338"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(591753, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ann_df[ann_df['image_id']== 203564]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "rchEERCcBuqv",
        "outputId": "7ac64319-ba42-47cf-c785-fb3deb25be17"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     image_id    id                                            caption\n",
              "0      203564    37  A bicycle replica with a clock as the front wh...\n",
              "8      203564   181                    The bike has a clock as a tire.\n",
              "19     203564   478  A black metal bicycle with a clock inside the ...\n",
              "237    203564  6637  A bicycle figurine in which the front wheel is...\n",
              "246    203564  6802  A clock with the appearance of the wheel of a ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c0d91bd4-806a-47d7-a838-cf1bdc93e514\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>id</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>203564</td>\n",
              "      <td>37</td>\n",
              "      <td>A bicycle replica with a clock as the front wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>203564</td>\n",
              "      <td>181</td>\n",
              "      <td>The bike has a clock as a tire.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>203564</td>\n",
              "      <td>478</td>\n",
              "      <td>A black metal bicycle with a clock inside the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>203564</td>\n",
              "      <td>6637</td>\n",
              "      <td>A bicycle figurine in which the front wheel is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>203564</td>\n",
              "      <td>6802</td>\n",
              "      <td>A clock with the appearance of the wheel of a ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0d91bd4-806a-47d7-a838-cf1bdc93e514')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c0d91bd4-806a-47d7-a838-cf1bdc93e514 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c0d91bd4-806a-47d7-a838-cf1bdc93e514');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_list = data['images']\n",
        "image_df = pd.DataFrame.from_records(image_list)"
      ],
      "metadata": {
        "id": "Ak0nqwH_9PYS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuucjYnBCTyQ",
        "outputId": "f87015f7-1c88-4e6a-a77e-a5e15021e9c8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118287, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.merge(ann_df, image_df, how='inner', left_on='image_id', right_on='id')\n"
      ],
      "metadata": {
        "id": "x8_h3eqvDhhy"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Index(['image_id', 'id_x', 'caption', 'license', 'file_name', 'coco_url',\n",
        "#        'height', 'width', 'date_captured', 'flickr_url', 'id_y'],\n",
        "#       dtype='object')\n",
        "final_df = final_df.drop(['license', 'coco_url','height','width','date_captured','flickr_url','id_y'], axis=1)"
      ],
      "metadata": {
        "id": "hUxUem7NuF7h"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save df to csv\n",
        "final_df.to_csv('df_caption_set.csv')\n"
      ],
      "metadata": {
        "id": "xLxiCGsuuFzB"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df3[df3['image_id']== 203564]"
      ],
      "metadata": {
        "id": "9x5_D2XSDzQp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5Tt7IM25hZV"
      },
      "source": [
        "### 2.2 Clean the captions\n",
        "\n",
        "Create a cleaned version of each caption. If using dataframes, we suggest saving the cleaned captions in a new column; otherwise, if you are storing your data in some other way, create data structures as needed. \n",
        "\n",
        "**A cleaned caption should be all lowercase, and consist of only alphabet characters.**\n",
        "\n",
        "Print out 10 original captions next to their cleaned versions to facilitate marking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "8PFXEvpE5hZW"
      },
      "outputs": [],
      "source": [
        "final_df['clean_caption'] = final_df.loc[:, 'caption']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "V8oTXenS5hZW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "8cebc1ad-392b-49d9-e8d4-ec07c30e7638"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        image_id    id_x                                            caption  \\\n",
              "591733    410182  829245                 a plate of various food on a table   \n",
              "591734    410182  829400       A plate of sandwiches and some french fries.   \n",
              "591735    410182  829537  Plate with fries and three sandwiches topped w...   \n",
              "591736    410182  829629  Slider sandwiches served with fries are displa...   \n",
              "591737    410182  829658  A long plate hold some fries with some sliders...   \n",
              "591738    383652  829299   A full fridge door sits open in an empty kitchen   \n",
              "591739    383652  829505       A open refrigerator door in a small kitchen.   \n",
              "591740    383652  829529  A refeigerator door is left open to shine its ...   \n",
              "591741    383652  829574  a refrigerator stocked full and left open with...   \n",
              "591742    383652  829633  The light from the open refrigerator fills the...   \n",
              "591743    133071  829358  Bread, sour cream, and guacamole are arrayed o...   \n",
              "591744    133071  829471           A white plate of food on a dining table.   \n",
              "591745    133071  829655  a slice of bread is covered with a sour cream ...   \n",
              "591746    133071  829693  White Plate with a lot of guacamole and an ext...   \n",
              "591747    133071  829717      A dinner plate has a lemon wedge garnishment.   \n",
              "591748    180285  829395       A couple of women with some stuffed animals.   \n",
              "591749    180285  829607     Fans pose with stuffed animals at an ice rink.   \n",
              "591750    180285  829636  Two women smiling together, one holds a stuffe...   \n",
              "591751    180285  829653  Two women smile for the camea while posing iwt...   \n",
              "591752    180285  829665       Two women sit and pose with stuffed animals.   \n",
              "\n",
              "               file_name                                      clean_caption  \n",
              "591733  000000410182.jpg                 a plate of various food on a table  \n",
              "591734  000000410182.jpg       a plate of sandwiches and some french fries.  \n",
              "591735  000000410182.jpg  plate with fries and three sandwiches topped w...  \n",
              "591736  000000410182.jpg  slider sandwiches served with fries are displa...  \n",
              "591737  000000410182.jpg  a long plate hold some fries with some sliders...  \n",
              "591738  000000383652.jpg   a full fridge door sits open in an empty kitchen  \n",
              "591739  000000383652.jpg       a open refrigerator door in a small kitchen.  \n",
              "591740  000000383652.jpg  a refeigerator door is left open to shine its ...  \n",
              "591741  000000383652.jpg  a refrigerator stocked full and left open with...  \n",
              "591742  000000383652.jpg  the light from the open refrigerator fills the...  \n",
              "591743  000000133071.jpg  bread, sour cream, and guacamole are arrayed o...  \n",
              "591744  000000133071.jpg           a white plate of food on a dining table.  \n",
              "591745  000000133071.jpg  a slice of bread is covered with a sour cream ...  \n",
              "591746  000000133071.jpg  white plate with a lot of guacamole and an ext...  \n",
              "591747  000000133071.jpg      a dinner plate has a lemon wedge garnishment.  \n",
              "591748  000000180285.jpg       a couple of women with some stuffed animals.  \n",
              "591749  000000180285.jpg     fans pose with stuffed animals at an ice rink.  \n",
              "591750  000000180285.jpg  two women smiling together, one holds a stuffe...  \n",
              "591751  000000180285.jpg  two women smile for the camea while posing iwt...  \n",
              "591752  000000180285.jpg       two women sit and pose with stuffed animals.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d1b37620-96b5-4649-acd8-cd2b18a9c24a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>id_x</th>\n",
              "      <th>caption</th>\n",
              "      <th>file_name</th>\n",
              "      <th>clean_caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>591733</th>\n",
              "      <td>410182</td>\n",
              "      <td>829245</td>\n",
              "      <td>a plate of various food on a table</td>\n",
              "      <td>000000410182.jpg</td>\n",
              "      <td>a plate of various food on a table</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591734</th>\n",
              "      <td>410182</td>\n",
              "      <td>829400</td>\n",
              "      <td>A plate of sandwiches and some french fries.</td>\n",
              "      <td>000000410182.jpg</td>\n",
              "      <td>a plate of sandwiches and some french fries.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591735</th>\n",
              "      <td>410182</td>\n",
              "      <td>829537</td>\n",
              "      <td>Plate with fries and three sandwiches topped w...</td>\n",
              "      <td>000000410182.jpg</td>\n",
              "      <td>plate with fries and three sandwiches topped w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591736</th>\n",
              "      <td>410182</td>\n",
              "      <td>829629</td>\n",
              "      <td>Slider sandwiches served with fries are displa...</td>\n",
              "      <td>000000410182.jpg</td>\n",
              "      <td>slider sandwiches served with fries are displa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591737</th>\n",
              "      <td>410182</td>\n",
              "      <td>829658</td>\n",
              "      <td>A long plate hold some fries with some sliders...</td>\n",
              "      <td>000000410182.jpg</td>\n",
              "      <td>a long plate hold some fries with some sliders...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591738</th>\n",
              "      <td>383652</td>\n",
              "      <td>829299</td>\n",
              "      <td>A full fridge door sits open in an empty kitchen</td>\n",
              "      <td>000000383652.jpg</td>\n",
              "      <td>a full fridge door sits open in an empty kitchen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591739</th>\n",
              "      <td>383652</td>\n",
              "      <td>829505</td>\n",
              "      <td>A open refrigerator door in a small kitchen.</td>\n",
              "      <td>000000383652.jpg</td>\n",
              "      <td>a open refrigerator door in a small kitchen.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591740</th>\n",
              "      <td>383652</td>\n",
              "      <td>829529</td>\n",
              "      <td>A refeigerator door is left open to shine its ...</td>\n",
              "      <td>000000383652.jpg</td>\n",
              "      <td>a refeigerator door is left open to shine its ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591741</th>\n",
              "      <td>383652</td>\n",
              "      <td>829574</td>\n",
              "      <td>a refrigerator stocked full and left open with...</td>\n",
              "      <td>000000383652.jpg</td>\n",
              "      <td>a refrigerator stocked full and left open with...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591742</th>\n",
              "      <td>383652</td>\n",
              "      <td>829633</td>\n",
              "      <td>The light from the open refrigerator fills the...</td>\n",
              "      <td>000000383652.jpg</td>\n",
              "      <td>the light from the open refrigerator fills the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591743</th>\n",
              "      <td>133071</td>\n",
              "      <td>829358</td>\n",
              "      <td>Bread, sour cream, and guacamole are arrayed o...</td>\n",
              "      <td>000000133071.jpg</td>\n",
              "      <td>bread, sour cream, and guacamole are arrayed o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591744</th>\n",
              "      <td>133071</td>\n",
              "      <td>829471</td>\n",
              "      <td>A white plate of food on a dining table.</td>\n",
              "      <td>000000133071.jpg</td>\n",
              "      <td>a white plate of food on a dining table.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591745</th>\n",
              "      <td>133071</td>\n",
              "      <td>829655</td>\n",
              "      <td>a slice of bread is covered with a sour cream ...</td>\n",
              "      <td>000000133071.jpg</td>\n",
              "      <td>a slice of bread is covered with a sour cream ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591746</th>\n",
              "      <td>133071</td>\n",
              "      <td>829693</td>\n",
              "      <td>White Plate with a lot of guacamole and an ext...</td>\n",
              "      <td>000000133071.jpg</td>\n",
              "      <td>white plate with a lot of guacamole and an ext...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591747</th>\n",
              "      <td>133071</td>\n",
              "      <td>829717</td>\n",
              "      <td>A dinner plate has a lemon wedge garnishment.</td>\n",
              "      <td>000000133071.jpg</td>\n",
              "      <td>a dinner plate has a lemon wedge garnishment.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591748</th>\n",
              "      <td>180285</td>\n",
              "      <td>829395</td>\n",
              "      <td>A couple of women with some stuffed animals.</td>\n",
              "      <td>000000180285.jpg</td>\n",
              "      <td>a couple of women with some stuffed animals.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591749</th>\n",
              "      <td>180285</td>\n",
              "      <td>829607</td>\n",
              "      <td>Fans pose with stuffed animals at an ice rink.</td>\n",
              "      <td>000000180285.jpg</td>\n",
              "      <td>fans pose with stuffed animals at an ice rink.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591750</th>\n",
              "      <td>180285</td>\n",
              "      <td>829636</td>\n",
              "      <td>Two women smiling together, one holds a stuffe...</td>\n",
              "      <td>000000180285.jpg</td>\n",
              "      <td>two women smiling together, one holds a stuffe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591751</th>\n",
              "      <td>180285</td>\n",
              "      <td>829653</td>\n",
              "      <td>Two women smile for the camea while posing iwt...</td>\n",
              "      <td>000000180285.jpg</td>\n",
              "      <td>two women smile for the camea while posing iwt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591752</th>\n",
              "      <td>180285</td>\n",
              "      <td>829665</td>\n",
              "      <td>Two women sit and pose with stuffed animals.</td>\n",
              "      <td>000000180285.jpg</td>\n",
              "      <td>two women sit and pose with stuffed animals.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1b37620-96b5-4649-acd8-cd2b18a9c24a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d1b37620-96b5-4649-acd8-cd2b18a9c24a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d1b37620-96b5-4649-acd8-cd2b18a9c24a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "final_df['clean_caption'] = final_df['clean_caption'].map(lambda x: x.lower() if isinstance(x,str) else x)\n",
        "final_df.tail(20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df['clean_caption'] = final_df['clean_caption'].str.replace(r'[^\\w\\s]+', '')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbUh00FbyyrS",
        "outputId": "f2244dec-3722-460a-f845-67e0a34a38e8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-60-1c48044e704d>:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  final_df['clean_caption'] = final_df['clean_caption'].str.replace(r'[^\\w\\s]+', '')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.tail(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "rNTUx8CdzG5C",
        "outputId": "1da1227e-30d6-4c45-ff1f-7aaf5231ce12"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        image_id    id_x                                            caption  \\\n",
              "591743    133071  829358  Bread, sour cream, and guacamole are arrayed o...   \n",
              "591744    133071  829471           A white plate of food on a dining table.   \n",
              "591745    133071  829655  a slice of bread is covered with a sour cream ...   \n",
              "591746    133071  829693  White Plate with a lot of guacamole and an ext...   \n",
              "591747    133071  829717      A dinner plate has a lemon wedge garnishment.   \n",
              "591748    180285  829395       A couple of women with some stuffed animals.   \n",
              "591749    180285  829607     Fans pose with stuffed animals at an ice rink.   \n",
              "591750    180285  829636  Two women smiling together, one holds a stuffe...   \n",
              "591751    180285  829653  Two women smile for the camea while posing iwt...   \n",
              "591752    180285  829665       Two women sit and pose with stuffed animals.   \n",
              "\n",
              "               file_name                                      clean_caption  \n",
              "591743  000000133071.jpg  bread sour cream and guacamole are arrayed on ...  \n",
              "591744  000000133071.jpg            a white plate of food on a dining table  \n",
              "591745  000000133071.jpg  a slice of bread is covered with a sour cream ...  \n",
              "591746  000000133071.jpg  white plate with a lot of guacamole and an ext...  \n",
              "591747  000000133071.jpg       a dinner plate has a lemon wedge garnishment  \n",
              "591748  000000180285.jpg        a couple of women with some stuffed animals  \n",
              "591749  000000180285.jpg      fans pose with stuffed animals at an ice rink  \n",
              "591750  000000180285.jpg  two women smiling together one holds a stuffed...  \n",
              "591751  000000180285.jpg  two women smile for the camea while posing iwt...  \n",
              "591752  000000180285.jpg        two women sit and pose with stuffed animals  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ed02f42d-d796-434b-b33e-27b2d817ebb9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>id_x</th>\n",
              "      <th>caption</th>\n",
              "      <th>file_name</th>\n",
              "      <th>clean_caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>591743</th>\n",
              "      <td>133071</td>\n",
              "      <td>829358</td>\n",
              "      <td>Bread, sour cream, and guacamole are arrayed o...</td>\n",
              "      <td>000000133071.jpg</td>\n",
              "      <td>bread sour cream and guacamole are arrayed on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591744</th>\n",
              "      <td>133071</td>\n",
              "      <td>829471</td>\n",
              "      <td>A white plate of food on a dining table.</td>\n",
              "      <td>000000133071.jpg</td>\n",
              "      <td>a white plate of food on a dining table</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591745</th>\n",
              "      <td>133071</td>\n",
              "      <td>829655</td>\n",
              "      <td>a slice of bread is covered with a sour cream ...</td>\n",
              "      <td>000000133071.jpg</td>\n",
              "      <td>a slice of bread is covered with a sour cream ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591746</th>\n",
              "      <td>133071</td>\n",
              "      <td>829693</td>\n",
              "      <td>White Plate with a lot of guacamole and an ext...</td>\n",
              "      <td>000000133071.jpg</td>\n",
              "      <td>white plate with a lot of guacamole and an ext...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591747</th>\n",
              "      <td>133071</td>\n",
              "      <td>829717</td>\n",
              "      <td>A dinner plate has a lemon wedge garnishment.</td>\n",
              "      <td>000000133071.jpg</td>\n",
              "      <td>a dinner plate has a lemon wedge garnishment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591748</th>\n",
              "      <td>180285</td>\n",
              "      <td>829395</td>\n",
              "      <td>A couple of women with some stuffed animals.</td>\n",
              "      <td>000000180285.jpg</td>\n",
              "      <td>a couple of women with some stuffed animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591749</th>\n",
              "      <td>180285</td>\n",
              "      <td>829607</td>\n",
              "      <td>Fans pose with stuffed animals at an ice rink.</td>\n",
              "      <td>000000180285.jpg</td>\n",
              "      <td>fans pose with stuffed animals at an ice rink</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591750</th>\n",
              "      <td>180285</td>\n",
              "      <td>829636</td>\n",
              "      <td>Two women smiling together, one holds a stuffe...</td>\n",
              "      <td>000000180285.jpg</td>\n",
              "      <td>two women smiling together one holds a stuffed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591751</th>\n",
              "      <td>180285</td>\n",
              "      <td>829653</td>\n",
              "      <td>Two women smile for the camea while posing iwt...</td>\n",
              "      <td>000000180285.jpg</td>\n",
              "      <td>two women smile for the camea while posing iwt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591752</th>\n",
              "      <td>180285</td>\n",
              "      <td>829665</td>\n",
              "      <td>Two women sit and pose with stuffed animals.</td>\n",
              "      <td>000000180285.jpg</td>\n",
              "      <td>two women sit and pose with stuffed animals</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed02f42d-d796-434b-b33e-27b2d817ebb9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ed02f42d-d796-434b-b33e-27b2d817ebb9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ed02f42d-d796-434b-b33e-27b2d817ebb9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "alphabet = string.ascii_letters+string.punctuation\n",
        "final_df.clean_caption.str.strip(alphabet).astype(bool).any()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAFxaA2V0-AO",
        "outputId": "814383f8-e3eb-4bcb-e985-98792c64a995"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "ApBDA_8g2ELS",
        "outputId": "91bb0844-4923-45d4-e808-4823e2d80012"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        image_id    id_x                                            caption  \\\n",
              "0         203564      37  A bicycle replica with a clock as the front wh...   \n",
              "1         203564     181                    The bike has a clock as a tire.   \n",
              "2         203564     478  A black metal bicycle with a clock inside the ...   \n",
              "3         203564    6637  A bicycle figurine in which the front wheel is...   \n",
              "4         203564    6802  A clock with the appearance of the wheel of a ...   \n",
              "...          ...     ...                                                ...   \n",
              "591748    180285  829395       A couple of women with some stuffed animals.   \n",
              "591749    180285  829607     Fans pose with stuffed animals at an ice rink.   \n",
              "591750    180285  829636  Two women smiling together, one holds a stuffe...   \n",
              "591751    180285  829653  Two women smile for the camea while posing iwt...   \n",
              "591752    180285  829665       Two women sit and pose with stuffed animals.   \n",
              "\n",
              "               file_name                                      clean_caption  \n",
              "0       000000203564.jpg  a bicycle replica with a clock as the front wheel  \n",
              "1       000000203564.jpg                     the bike has a clock as a tire  \n",
              "2       000000203564.jpg  a black metal bicycle with a clock inside the ...  \n",
              "3       000000203564.jpg  a bicycle figurine in which the front wheel is...  \n",
              "4       000000203564.jpg  a clock with the appearance of the wheel of a ...  \n",
              "...                  ...                                                ...  \n",
              "591748  000000180285.jpg        a couple of women with some stuffed animals  \n",
              "591749  000000180285.jpg      fans pose with stuffed animals at an ice rink  \n",
              "591750  000000180285.jpg  two women smiling together one holds a stuffed...  \n",
              "591751  000000180285.jpg  two women smile for the camea while posing iwt...  \n",
              "591752  000000180285.jpg        two women sit and pose with stuffed animals  \n",
              "\n",
              "[591753 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f7dea09-038a-4223-bd50-0eb63dd3a381\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>id_x</th>\n",
              "      <th>caption</th>\n",
              "      <th>file_name</th>\n",
              "      <th>clean_caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>203564</td>\n",
              "      <td>37</td>\n",
              "      <td>A bicycle replica with a clock as the front wh...</td>\n",
              "      <td>000000203564.jpg</td>\n",
              "      <td>a bicycle replica with a clock as the front wheel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>203564</td>\n",
              "      <td>181</td>\n",
              "      <td>The bike has a clock as a tire.</td>\n",
              "      <td>000000203564.jpg</td>\n",
              "      <td>the bike has a clock as a tire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>203564</td>\n",
              "      <td>478</td>\n",
              "      <td>A black metal bicycle with a clock inside the ...</td>\n",
              "      <td>000000203564.jpg</td>\n",
              "      <td>a black metal bicycle with a clock inside the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>203564</td>\n",
              "      <td>6637</td>\n",
              "      <td>A bicycle figurine in which the front wheel is...</td>\n",
              "      <td>000000203564.jpg</td>\n",
              "      <td>a bicycle figurine in which the front wheel is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>203564</td>\n",
              "      <td>6802</td>\n",
              "      <td>A clock with the appearance of the wheel of a ...</td>\n",
              "      <td>000000203564.jpg</td>\n",
              "      <td>a clock with the appearance of the wheel of a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591748</th>\n",
              "      <td>180285</td>\n",
              "      <td>829395</td>\n",
              "      <td>A couple of women with some stuffed animals.</td>\n",
              "      <td>000000180285.jpg</td>\n",
              "      <td>a couple of women with some stuffed animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591749</th>\n",
              "      <td>180285</td>\n",
              "      <td>829607</td>\n",
              "      <td>Fans pose with stuffed animals at an ice rink.</td>\n",
              "      <td>000000180285.jpg</td>\n",
              "      <td>fans pose with stuffed animals at an ice rink</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591750</th>\n",
              "      <td>180285</td>\n",
              "      <td>829636</td>\n",
              "      <td>Two women smiling together, one holds a stuffe...</td>\n",
              "      <td>000000180285.jpg</td>\n",
              "      <td>two women smiling together one holds a stuffed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591751</th>\n",
              "      <td>180285</td>\n",
              "      <td>829653</td>\n",
              "      <td>Two women smile for the camea while posing iwt...</td>\n",
              "      <td>000000180285.jpg</td>\n",
              "      <td>two women smile for the camea while posing iwt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591752</th>\n",
              "      <td>180285</td>\n",
              "      <td>829665</td>\n",
              "      <td>Two women sit and pose with stuffed animals.</td>\n",
              "      <td>000000180285.jpg</td>\n",
              "      <td>two women sit and pose with stuffed animals</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>591753 rows  5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f7dea09-038a-4223-bd50-0eb63dd3a381')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0f7dea09-038a-4223-bd50-0eb63dd3a381 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0f7dea09-038a-4223-bd50-0eb63dd3a381');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Umb4VT5hZW"
      },
      "source": [
        "### 2.3  Split the data\n",
        "\n",
        "Split the data 70/10/20% into train/validation/test sets. **Be sure that each unique image (and all corresponding captions) only appear in a single set.**\n",
        "\n",
        "We provide the function below which, given a list of unique image IDs and a 3-split ratio, shuffles and returns  a split of the image IDs.\n",
        "\n",
        "If using a dataframe, ```df['image_id'].unique()``` will return the list of unique image IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "rue1LnaX5hZW"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "def split_ids(image_id_list, train=.7, valid=0.1, test=0.2):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        image_id_list (int list): list of unique image ids\n",
        "        train (float): train split size (between 0 - 1)\n",
        "        valid (float): valid split size (between 0 - 1)\n",
        "        test (float): test split size (between 0 - 1)\n",
        "    \"\"\"\n",
        "    list_copy = image_id_list.copy()\n",
        "    random.shuffle(list_copy)\n",
        "    \n",
        "    train_size = math.floor(len(list_copy) * train)\n",
        "    valid_size = math.floor(len(list_copy) * valid)\n",
        "    \n",
        "    return list_copy[:train_size], list_copy[train_size:(train_size + valid_size)], list_copy[(train_size + valid_size):]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df['image_id'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG8dPzvn4hea",
        "outputId": "a8d95d07-2b93-42ea-c516-3154e5cf717e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([203564, 322141,  16977, ..., 383652, 133071, 180285])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6XWiygT5hZX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggrsNGPk5hZX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3msz-_a5hZX"
      },
      "source": [
        "### 2.3 Building the vocabulary\n",
        "\n",
        "The vocabulary consists of all the possible words which can be used - both as input into the model, and as output predictions, and we will build it using the cleaned words found in the reference captions from the training set. In the vocabulary each unique word is mapped to a unique integer (a Python ```dictionary``` object).\n",
        "\n",
        "A ```Vocabulary``` object is provided for you below to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZDBfoa55hZY"
      },
      "outputs": [],
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\" Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
        "    def __init__(self):\n",
        "        # intially, set both the IDs and words to dictionaries with special tokens\n",
        "        self.word2idx = {'<pad>': 0, '<unk>': 1, '<end>': 2}\n",
        "        self.idx2word = {0: '<pad>', 1: '<unk>', 2: '<end>'}\n",
        "        self.idx = 3\n",
        "\n",
        "    def add_word(self, word):\n",
        "        # if the word does not already exist in the dictionary, add it\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            # increment the ID for the next word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        # if we try to access a word not in the dictionary, return the id for <unk>\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mxFuzhj5hZY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvs5b6M95hZY"
      },
      "source": [
        "Collect all words from the cleaned captions in the **training and validation sets**, ignoring any words which appear 3 times or less; this should leave you with roughly 2200 words (plus or minus is fine). As the vocabulary size affects the embedding layer dimensions, it is better not to add the very infrequently used words to the vocabulary.\n",
        "\n",
        "Create an instance of the ```Vocabulary()``` object and add all your words to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bPL4WQg5hZY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mzn9S_cv5hZY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uis6nR9N5hZZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYx-qVAX5hZZ"
      },
      "source": [
        "### 2.4 The Dataset and DataLoader\n",
        "\n",
        "Create a PyTorch ```Dataset``` class and a corresponding ```DataLoader``` for the inputs to the decoder. Create three sets: one each for training, validation, and test. Set ```shuffle=True``` for the training set DataLoader.\n",
        "\n",
        "The ```Dataset``` function ```__getitem__(self, index)``` should return three Tensors:\n",
        "\n",
        ">1. A Tensor of image features, dimension (1, 2048).\n",
        ">2. A Tensor of integer word ids representing the reference caption; use your ```Vocabulary``` object to convert each word in the caption to a word ID. Be sure to add the word ID for the ```<end>``` token at the end of each caption, then fill in the the rest of the caption with the ```<pad>``` token so that each caption has uniform lenth (max sequence length) of **47**.\n",
        ">3. A Tensor of integers representing the true lengths of every caption in the batch (include the ```<end>``` token in the count).\n",
        "\n",
        "\n",
        "Note that as each unique image has five or more (say, ```n```) reference captions, each image feature will appear ```n``` times, once in each unique (feature, caption) pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkIvy-SM5hZZ"
      },
      "outputs": [],
      "source": [
        "class COCO_Subset(Dataset):\n",
        "    \"\"\" COCO subset custom dataset, compatible with torch.utils.data.DataLoader. \"\"\"\n",
        "    \n",
        "    def __init__(self, df, features, vocab):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df: (dataframe or some other data structure/s you may prefer to use)\n",
        "            features: image features\n",
        "            vocab: vocabulary wrapper\n",
        "           \n",
        "        \"\"\"\n",
        "        \n",
        "        # TO COMPLETE\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Returns one data tuple (feature [1, 2048], target caption of word IDs [1, 47], and integer true caption length) \"\"\"   \n",
        "        \n",
        "       # TO COMPLETE\n",
        "    \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgkEEmPA5hZZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S66p-6d5hZZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2pYh9Rf5hZZ"
      },
      "source": [
        "Load one batch of the training set and print out the shape of each returned Tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVEaAwCH5hZZ"
      },
      "outputs": [],
      "source": [
        "# train_iter = iter(train_loader)\n",
        "# features, captions, lengths = train_iter.next()\n",
        "# print(features.shape)\n",
        "# print(captions.shape)\n",
        "# print(lengths.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ3naDXD5hZa"
      },
      "source": [
        "## 3 Train DecoderRNN [15 marks]\n",
        "\n",
        "### 3.1 Define the encoder model\n",
        "\n",
        "Read through the ```DecoderRNN``` model below. First, complete the decoder by adding an ```rnn``` layer to the decoder where indicated, using [the PyTorch API as reference](https://pytorch.org/docs/stable/nn.html#rnn).\n",
        "\n",
        "Keep all the default parameters except for ```batch_first```, which you may set to True.\n",
        "\n",
        "In particular, understand the meaning of ```pack_padded_sequence()``` as used in ```forward()```. Refer to the [PyTorch ```pack_padded_sequence()``` documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFbFxvIU5hZa"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=256, hidden_size=512, num_layers=1, max_seq_length=47):\n",
        "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        # we want a specific output size, which is the size of our embedding, so\n",
        "        # we feed our extracted features from the last fc layer (dimensions 1 x 2048)\n",
        "        # into a Linear layer to resize\n",
        "        self.resize = nn.Linear(2048, embed_size)\n",
        "        \n",
        "        # batch normalisation helps to speed up training\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "\n",
        "        # What is an embedding layer?\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # TO COMPLETE\n",
        "        # self.rnn = \n",
        "\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.max_seq_length = max_seq_length\n",
        "        \n",
        "\n",
        "    def forward(self, features, captions, lengths):\n",
        "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
        "        embeddings = self.embed(captions)\n",
        "        im_features = self.resize(features)\n",
        "        im_features = self.bn(im_features)\n",
        "        \n",
        "        embeddings = torch.cat((im_features.unsqueeze(1), embeddings), 1)\n",
        "    \n",
        "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False) \n",
        "        # pack_padded_sequence returns a PackedSequence object, which contains two items: \n",
        "        # the packed data (data cut off at its true length and flattened into one list), and \n",
        "        # the batch_sizes, or the number of elements at each sequence step in the batch.\n",
        "        # For instance, given data [a, b, c] and [x] the PackedSequence would contain data \n",
        "        # [a, x, b, c] with batch_sizes=[2,1,1].\n",
        "        \n",
        "        hiddens, _ = self.rnn(packed)\n",
        "        outputs = self.linear(hiddens.data) # hiddens[0]\n",
        "        return outputs\n",
        "    \n",
        "    \n",
        "    def sample(self, features, states=None):\n",
        "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
        "        sampled_ids = []\n",
        "\n",
        "        inputs = self.bn(self.resize(features)).unsqueeze(1)\n",
        "        for i in range(self.max_seq_length):\n",
        "            hiddens, states = self.rnn(inputs, states)  # hiddens: (batch_size, 1, hidden_size)\n",
        "            outputs = self.linear(hiddens.squeeze(1))   # outputs:  (batch_size, vocab_size)\n",
        "            _, predicted = outputs.max(1)               # predicted: (batch_size)\n",
        "            sampled_ids.append(predicted)\n",
        "            inputs = self.embed(predicted)              # inputs: (batch_size, embed_size)\n",
        "            inputs = inputs.unsqueeze(1)                # inputs: (batch_size, 1, embed_size)\n",
        "        sampled_ids = torch.stack(sampled_ids, 1)       # sampled_ids: (batch_size, max_seq_length)\n",
        "        return sampled_ids\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCLsCeEB5hZa"
      },
      "outputs": [],
      "source": [
        "# instantiate decoder\n",
        "decoder = DecoderRNN(len(vocab)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga9bFPnu5hZa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll5JI91J5hZb"
      },
      "source": [
        "### 3.2 Train the decoder\n",
        "\n",
        "Train the decoder by passing the features, reference captions, and targets to the decoder, then computing loss based on the outputs and the targets. Note that when passing the targets and model outputs to the loss function, the targets will also need to be formatted using ```pack_padded_sequence()```.\n",
        "\n",
        "We recommend a batch size of around 64 (though feel free to adjust as necessary for your hardware).\n",
        "\n",
        "**We strongly recommend saving a checkpoint of your trained model after training so you don't need to re-train multiple times.**\n",
        "\n",
        "Display a graph of training and validation loss over epochs to justify your stopping point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmVVrKd45hZb"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib0LCGj_5hZb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evsh9miO5hZb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUqfK8QI5hZb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXzlLbstCE7f"
      },
      "source": [
        "## 4 Generate predictions on test data [8 marks]\n",
        "\n",
        "Display 5 sample test images containing different objects, along with your models generated captions and all the reference captions for each.\n",
        "\n",
        "> Remember that everything **displayed** in the submitted notebook and .html file will be marked, so be sure to run all relevant cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-O-Vz2l5hZb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBU8-UsD5hZc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7np733bd5hZc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVfEgbC4I_dE"
      },
      "source": [
        "## 5 Caption evaluation using BLEU score [10 marks]\n",
        "\n",
        "There are different methods for measuring the performance of image to text models. We will evaluate our model by measuring the text similarity between the generated caption and the reference captions, using two commonly used methods. Ther first method is known as *Bilingual Evaluation Understudy (BLEU)*.\n",
        "\n",
        "###  5.1 BLEU score\n",
        "\n",
        "\n",
        "One common way of comparing a generated text to a reference text is using BLEU. This article gives a good intuition to how the BLEU score is computed: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/, and you may find an implementation online to use. One option is the NLTK implementation `nltk.translate.bleu_score` here: https://www.nltk.org/api/nltk.translate.bleu_score.html\n",
        "\n",
        "\n",
        "> **Tip:** BLEU scores can be weighted by ith-gram. Check that your scores make sense; and feel free to use a weighting that best matches the data. We will not be looking for specific score ranges; rather we will check that the scores are reasonable and meaningful given the captions.\n",
        "\n",
        "Write the code to evaluate the trained model on the complete test set and calculate the BLEU score using the predictions, compared against all five references captions. \n",
        "\n",
        "Display a histogram of the distribution of scores over the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xypfUN7y4CKI"
      },
      "outputs": [],
      "source": [
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAXrOzjE5hZc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPnCbFp25hZc"
      },
      "source": [
        "### 5.2 BLEU score examples\n",
        "\n",
        "Find one sample with high BLEU score and one with a low score, and display the model's predicted sentences, the BLEU scores, and the 5 reference captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3RqgXUq5hZc"
      },
      "outputs": [],
      "source": [
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIuM7H5Q5hZc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VQKNo384CKP"
      },
      "source": [
        "## 6 Caption evaluation using cosine similarity [12 marks]\n",
        "\n",
        "###  6.1 Cosine similarity\n",
        "\n",
        "The cosine similarity measures the cosine of the angle between two vectors in n-dimensional space. The smaller the angle, the greater the similarity.\n",
        "\n",
        "To use the cosine similarity to measure the similarity between the generated caption and the reference captions: \n",
        "\n",
        "* Find the embedding vector of each word in the caption \n",
        "* Compute the average vector for each caption \n",
        "* Compute the cosine similarity score between the average vector of the generated caption and average vector of each reference caption\n",
        "* Compute the average of these scores \n",
        "\n",
        "Calculate the cosine similarity using the model's predictions over the whole test set. \n",
        "\n",
        "Display a histogram of the distribution of scores over the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4kaOxNY5hZd"
      },
      "outputs": [],
      "source": [
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aECpoyvk5hZd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19TErXv45hZd"
      },
      "source": [
        "#### 6.2 Cosine similarity examples \n",
        "\n",
        "Find one sample with high cosine similarity score and one with a low score, and display the model's predicted sentences, the cosine similarity scores, and the 5 reference captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRINvQc-5hZd"
      },
      "outputs": [],
      "source": [
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbVvSjcW5hZd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDmwrp-w4CKR"
      },
      "source": [
        "## 7 Comparing BLEU and Cosine similarity [16 marks]\n",
        "\n",
        "### 7.1 Test set distribution of scores\n",
        "\n",
        "Compare the models performance on the test set evaluated using BLEU and cosine similarity and discuss some weaknesses and strengths of each method (explain in words, in a text box below). \n",
        "\n",
        "Please note, to compare the average test scores, you need to rescale the Cosine similarity scores [-1 to 1] to match the range of BLEU method [0.0 - 1.0]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2O8TZG74CKS"
      },
      "outputs": [],
      "source": [
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qigb0A9F4CKV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2fHKGoo5hZe"
      },
      "source": [
        " ### 7.2 Analysis of individual examples\n",
        " \n",
        "Find and display one example where both methods give similar scores and another example where they do not and discuss. Include both scores, predicted captions, and reference captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixoMFjun5hZe"
      },
      "outputs": [],
      "source": [
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHFyGELx5hZe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPb2D3az5hZe"
      },
      "source": [
        "### Overall quality [5 marks]\n",
        "\n",
        "See the top of the notebook for submission instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK_S95Rk5hZf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6405531de5294830a30ef1432477ca57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f523702c2d234f78b65b9893c6fc2992",
              "IPY_MODEL_c8bf132ab08041e5b3a8552f86b1fd49",
              "IPY_MODEL_1222b94434b7480798edc5671bd22f85"
            ],
            "layout": "IPY_MODEL_a3b9a8a2f1fc412e8e87f6d8c6e547c4"
          }
        },
        "f523702c2d234f78b65b9893c6fc2992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a069b7fd02164fcc8c9fa9a3f0c1f873",
            "placeholder": "",
            "style": "IPY_MODEL_b91188e4889f4238929c218967e2adbc",
            "value": "100%"
          }
        },
        "c8bf132ab08041e5b3a8552f86b1fd49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_494fc511ed5042c3a18e2711179daba3",
            "max": 241627721,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_107ba6832156498f8b538a0b66b4a316",
            "value": 241627721
          }
        },
        "1222b94434b7480798edc5671bd22f85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f74db6225ff48c49cd8bce99186f06f",
            "placeholder": "",
            "style": "IPY_MODEL_b5ef3d0bd8204a619f4dea96940af541",
            "value": " 230M/230M [00:02&lt;00:00, 77.5MB/s]"
          }
        },
        "a3b9a8a2f1fc412e8e87f6d8c6e547c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a069b7fd02164fcc8c9fa9a3f0c1f873": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b91188e4889f4238929c218967e2adbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "494fc511ed5042c3a18e2711179daba3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "107ba6832156498f8b538a0b66b4a316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f74db6225ff48c49cd8bce99186f06f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5ef3d0bd8204a619f4dea96940af541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}